# -*- coding: utf-8 -*-
"""
Created on Wed Aug 16 17:14:37 2023

@author: Alejandra Fuentes
"""


#Importing numpy, tensorflow, keras and other useful libraries  
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.models import Model
from keras.layers import Dense, Dropout, Flatten, Conv2D, ReLU,MaxPooling2D, Normalization, Conv1D, InputLayer, Activation, BatchNormalization, ELU, MaxPooling1D, Normalization,GlobalAveragePooling1D,AveragePooling1D,LeakyReLU,PReLU,GlobalMaxPooling1D
from keras.constraints import NonNeg
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder,MinMaxScaler
from numpy import mean,std
from sklearn.metrics import confusion_matrix

#data (x input spectra ) and labels (radiosensitive vs radioresistant)
data = pd.read_csv(r'C:\Users\Alejandra\OneDrive - UBC\New folder\modifiedGradCAMlit\PlosOneAnalystzscoresmoother.csv',header=None)
labels = pd.read_csv(r'C:\Users\Alejandra\OneDrive - UBC\New folder\modifiedGradCAMlit\PlosOneAnalystLabels.csv',header=None) 

#Convert the category label names (e.g, radiosensitive versus radioresistant) to vectors for binary classification
#our classes will be: [1 0] and [0 1]
le = LabelEncoder()
le.fit(labels)
classes = le.classes_
print(le.classes_)
num_classes = len(classes)
y_data_raw = np.copy(labels)
y_data = le.transform(y_data_raw)
y_data = tf.keras.utils.to_categorical(y_data)


#Random number generator: set seed for reproducible results
seedy=1300
np.random.seed(seedy)
tf.random.set_seed(seedy) 


#Splitting my data and their labels into randomly defined training and testing sets
# For now, 20% of the total data is set for testing 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, y_data, test_size=0.2)
num_classes = 2

# Convert the 2D data matrix  to 3D array so that input is acceptable to the CNN. 
#From num of spectra x num of points matrix to a num of spectra x num of points x 1  3-Dmatrix (only one channel)
X_train = np.expand_dims(X_train, -1)
X_test = np.expand_dims(X_test, -1)
Alldata=np.expand_dims(data,-1)

#Define CNN number and size of filters, dropout ratio, number of neurons in first FC layer, minibatch size.
numfiltersly1=32
filtersizely1=8
numfiltersly2=64
filtersizely2=filtersizely1
#FCsize=10 for alternative cnn architectures presented in Supplementary Figures
DropRatio=0.2 
minibatchsize=50 

model=keras.models.Sequential()
model.add(Conv1D(filters=numfiltersly1, kernel_size=filtersizely1,strides=2,dilation_rate=1,use_bias=False,activation=None,input_shape=(582,1),padding='same')) #padding
model.add(BatchNormalization())
model.add(ReLU())
model.add(Conv1D(filters=numfiltersly2, kernel_size=filtersizely2,strides=3,dilation_rate=1,use_bias=False,activation=None,padding='same')) 
model.add(BatchNormalization())
model.add(ReLU())
model.add(keras.layers.Flatten())
#model.add(keras.layers.Dense(FCsize))
model.add(Dropout(DropRatio))
model.add(keras.layers.Dense(num_classes,use_bias=True,activation='softmax'))
opt = keras.optimizers.Adam(learning_rate=0.0001,beta_1=0.9,beta_2=0.999,epsilon=1e-08,amsgrad=True)
model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])
history = model.fit(X_train,y_train,batch_size=minibatchsize,epochs=32,validation_split=0.125) 
model.summary() 


# Plot training progress vs epochs 
print(history.history.keys())
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

#Evaluate the CNN with the testing set: I get the test accuracy and also the class scores for each test sample
test_loss,test_accuracy=model.evaluate(X_test,y_test)
Testclassscore=model.predict(X_test)

#Confusion matrix from which we can ger
y_prediction = model.predict(X_test)
y_prediction = np.argmax (y_prediction, axis = 1)
Y_test=np.argmax(y_test, axis=1)
result = confusion_matrix(Y_test, y_prediction , normalize='pred')
print(result)

#model.save('cnnmodel_seednum.h5') 



#GRADCAM FORMULATION 
def relu(x):
    return(np.maximum(0, x))


def grad_cam(layer_name, data, model):
    
    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])
    
    last_conv_layer_output, preds = grad_model(data)

    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(data)
        #print(last_conv_layer_output.shape)
        #print(preds.shape) 
        #print(preds) 
        pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]
        #print(pred_index)
        #print(class_channel)
    
    grads = tape.gradient(class_channel, last_conv_layer_output)
    #print(grads.shape)
    last_conv_layer_output = last_conv_layer_output[0]
    ##print(last_conv_layer_output.shape)
    pooled_grads = tf.reduce_mean(grads, axis=(0))
    #print(pooled_grads.shape)#use print to see intermediate outputs
    heatmap = np.multiply(last_conv_layer_output,pooled_grads)
    #print(heatmap.shape)
    heatmap = tf.reduce_mean(heatmap, axis=(1))
    #print(heatmap.shape)
    heatmap =relu(np.expand_dims(heatmap,0))
    #print(heatmap.shape)
    # For visualization purpose, we will also normalize the heatmap between 0 & 1
    heatmap=(heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))  
    #print(heatmap.shape)
    return heatmap
  

# Remove last layer's softmax - 
#model.layers[-1].activation = None
#print(model.layers[-1].activation)



#sample Grad-CAM heatmaps
%matplotlib inline
for i in range(1,30):
    example = X_test[i]
    exp = grad_cam("re_lu_1", np.expand_dims(example,axis=0), model)
    plt.imshow(exp, cmap='jet', aspect="auto", interpolation='bilinear',extent=[0,582,-1.5,5],
           vmin=exp.min(), vmax=exp.max(), alpha=1.0)
    plt.plot(example,'w')
    pred = model.predict(np.expand_dims(example,axis=0))
    plt.title('GradCAM\nPrediction: '+str(pred[0])+'\nLabel: '+str(y_test[i]))
    plt.colorbar()
    plt.show()
    
